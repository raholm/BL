---
title: "Bayesian Learning"
subtitle: Lab 4
author: "Emil K Svensson and Rasmus Holm"
date: '`r Sys.Date()`'
output:
    pdf_document:
        includes:
            in_header: styles.sty
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Question 1

```{r}
bid <- read.table("../data/eBayNumberOfBidderData.dat",header = TRUE)
```

## a)

```{r}
poiglm <- glm(nBids ~ . - 1, data = bid, family = poisson(link = "log"))
summary(poiglm)
```

## b)

```{r}
library(mvtnorm)

logprior <- function(beta, mu, sigma){
    dmvnorm(beta, mean = mu, sigma = sigma, log = TRUE)
}

loglikelihood <- function(beta, X, Y){
    linear_prediction <- t(X) %*% beta
    probabilities <- Y * linear_prediction - exp(linear_prediction)
    loglike <- sum(probabilities)

    ## if (abs(loglike) == Inf)
    ##     loglike = -20000

    loglike
}

## loglikelihood <- function(beta, X, Y) {
##   linear_prediction <- t(X) %*% beta
##   probs <- dpois(Y , lambda = exp(linear_prediction), log = TRUE)
##   sum(probs)
## }

logposterior <- function(beta, X, Y, prior_mu, prior_sigma){
    loglikelihood(beta, X, Y) + logprior(beta, prior_mu, prior_sigma)
}

X <- as.matrix(bid[,-1])
Y <- as.matrix(bid[,1])

mu <- rep(0, ncol(X))
sigma <- 100 * solve(t(X) %*% X)

optpost <- optim(par = matrix(rep(0, ncol(X)), ncol = 1),
                 fn = logposterior, method = "BFGS", hessian = TRUE,
                 X = t(X), Y = Y,
                 prior_mu= mu, prior_sigma = sigma,
                 control=list(fnscale=-1))
hessian <- optpost$hessian
```

```{r}
as.vector(optpost$par)
as.vector(coef(poiglm))
```

## c)

```{r}
targetdensity <- function(theta, prior_mu, prior_sigma, X, Y, ...) {
    likelihood <- dpois(Y, lambda = exp(t(X) %*% t(theta)), log = TRUE)
    prior <- dmvnorm(theta, mean = prior_mu, sigma = prior_sigma, log = TRUE)
    sum(likelihood) + prior
}

proposaldensity <- function(x, mu, prop_sigma, ...){
    dmvnorm(x, mean = mu, sigma = prop_sigma, log = TRUE)
}

proposalsampler <- function(mu, prop_sigma, ...){
    matrix(rmvnorm(1, mean = mu, sigma = prop_sigma), nrow = 1)
}

metropolis_hastings <- function(prop_sampler, log_prop_func, log_targ_post_func, X0, iters, ...){
    x <- X0
    values <- matrix(0, ncol = length(X0), nrow = iters + 1)
    values[1,] <- X0

    alpha <- function(x, y, ...) {
        numerator <- log_targ_post_func(y, ...) + log_prop_func(x, y, ...)
        denominator <- log_targ_post_func(x, ...) + log_prop_func(y, x, ...)
        exp(numerator - denominator)
    }

    for (i in 1:iters) {
        y <- prop_sampler(x, ...)
        u <- runif(1)

        if (u < alpha(x, y, ...)) {
            x <- y
        }

        values[i+1,] <- x
    }

    values
}

iters <- 5000
X0 <- rep(0, times = ncol(X))

params <- list(
    prop_sampler = proposalsampler,
    log_prop_func = proposaldensity,
    log_targ_post_func = targetdensity,
    X0 = matrix(rep(0, times = ncol(X)), nrow = 1),
    iters = iters,
    X = t(X),
    Y = Y,
    prior_mu = rep(0, times = ncol(X)),
    prior_sigma = 100 * solve(t(X) %*% X),
    prop_sigma = 0.6 * -solve(hessian)
)

res <- do.call(metropolis_hastings, params)

colMeans(res)
```

## d

```{r}
Xpred <- matrix(c(1, 1, 1, 1, 0, 0, 0, 1, 0.5), nrow = 1)
predsmaples <- rpois(10000, lambda = exp(Xpred %*% t(res)))

mean(predsmaples == 0)
hist(predsmaples, breaks = 50)
```
